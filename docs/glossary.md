---
toc_depth: 1
---

# Glossary

## Introduction

This glossary defines key terms related to the Research Computing system to help researchers, administrators, help desks, and users effectively use our resources, and also serving as a helpful reference for understanding the system's terminology, listed alphabetically below.

 [A](#a) [**B**](#b) [**C**](#c) [**D**](#d) [**E**](#e) [**F**](#f) [**G**](#g) [**H**](#h) [**I**](#i) [**J**](#j) [**K**](#k) [**L**](#l) [**M**](#m) [**N**](#n) [**O**](#o) [**P**](#p) [**Q**](#q) [**R**](#r) [**S**](#s) [**T**](#t) [**U**](#u) [**V**](#v) [**W**](#w) [**X**](#x) [**Y**](#y) [**Z**](#z)

### A

**section:Account**
:   Refers to the user credentials that you use to log into the Research Computing systems.

**section:ACLs**
:   Access Control Lists (ACLs) is a mechanism used to define permissions for files and directories in an HPC system.

**section:Array Job**
:   A job type in HPC used to submit and execute a large number of identical or similar tasks in parallel, managed efficiently under a single job submission.

### B

**section:BlazerID**
:   The user name that will bed used to connect to any UAB system.

### C

**section:Cheaha**
:   A shared cluster computing environment for UAB researchers

**section:Compute node**
:   A dedicated server in an Cheaha cluster designed to perform computational tasks.

**section:Core**
:   Individual processing unit within a CPU that can execute tasks.

### D

**section:Data transfer**
:   The process of moving files between local systems and Cheaha, or between storage locations on the Research computing system.

### E

### F

### G

**section:Globus**
:    A data transfer tool that enables fast and secure research data transfers between our local systems and Cheaha system, or between storage locations on the Cheaha system.

**section: GPFS storage**
:   General Parallel File System (GPFS) storage provides scalable and distributed storage to manage large amounts of data efficiently. For example, Cheaha project directory is a GPFS storage.

**section:GPU**
:   Graphics Processing Units (GPUs), specialized hardware for parallel processing, often used in machine learning, deep learning, and other computationally intensive tasks.

### H

### I

**section:Instance**
:   A virtualized compute resource in the cloud, often running as a virtual machine, with allocated resources (such as CPU, memory, and storage resources).

### J

**section:Jobs**
:   Tasks submitted to the Cheaha system for execution, typically managed by a SLURM scheduler.

### K

**section:Key pairs**
:   These can be keys used to securely access virtual machines or cloud instances via SSH (a public key and private key). They can also be used as a username (access key) and password (secret key) to access LTS (Long-Term Storage).

**section:Keras**
:   A high level neural network API that runs on top of TensorFlow for training machine learning and deep learning models.

### L

**section:Login node**
:   An entry point to a Cheaha cluster for users.

**section:Long-Term Storage**
:   An S3 object-storage platform hosted at UAB which is designed to hold data that is not currently being used in analysis but should be kept for data sharing or reused for further analysis in the future.

### M

**section:MATLAB**
:   Application software for numerical computation, data analysis, and visualization.

**section:Memory**
:   The system's storage, typically called RAM, allocated to a job for temporary data storage during its execution.

**section:Modules**
:    A software package on HPC systems, allowing user to easily load/access specific versions of software applications and libraries.

### N

**section:Node**
:   A single computational unit in an HPC cluster, containing processors, and memory.

### O

**section:OOD**
:   Open OnDemand (OOD) is a web based portal that provides users with easy access to compute resources, file systems, and job management tools through a graphical interface, in HPC environment.

### P

**section:Partition**
:   A logical group of nodes that are organized based on their hardware, usage type, or priority.

**section:PyTorch**
:   An open source framework, developed by Meta, that provides tools for developing and training deep learning models.

### Q

**section:Quality of Service (QoS) Limits**
:   QoS limits allow us to balance usage and ensure fairness for all researchers using the cluster.

**section:Quota**
:   Limits on resources such as storage or computational time allocated to a user.

### R

**section:RStudio**
:   An integrated development environment (IDE) for R programming, used for statistical computing and data analysis.

### S

**section:Shared Storage**
:   A centralized storage space (e.g Cheaha project directory, or Shared LTS allocation) used for collaborative work by multiple users, managed by the PIs.

**section:SLURM**
:   Simple Linux Utility for Resource Management (SLURM), a popular workload manager in HPC systems. It schedules jobs based using resource requests such as number of CPUs, maximum memory (RAM) required per CPU, maximum run time, and more.

**section:State**
:   The current status of a job in the scheduler, such as pending, running, completed, or failed.

**section:Storage**
:   Resources allocated for storing data that includes home/user, project, scratch directories on Cheaha and LTS.

**section:Submit Jobs**
:   The process of queuing computational tasks to run on the HPC system using a job scheduler such as SLURM.

### T

**section:TensorFlow**
:   An open-source machine learning framework, developed by Google, commonly used for building and training deep learning models.

### U

### V

**section:Virtual Machine (VM)**
:   A software-based computer that provides a virtualized environment, that functions like a physical computer, for running applications or operating systems.

**section:Volumes**
:   Virtual storage devices used to persist data, often associated with Virtual Machines (VM) or cloud environments.

### W

### X

**section:XIAS account**
:   A guest user credential for non-UAB individuals who need access to our HPC system. It uses the guest's email as the username.

### Y

### Z
<!-- <glossary::section> -->
